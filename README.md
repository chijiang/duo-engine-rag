# LlamaIndex Dual-Engine RAG System

[![Project Status: Active](https://img.shields.io/badge/status-active-success.svg)](https://github.com/chijiang/duo-engine-rag.git)

This project implements a sophisticated Retrieval Augmented Generation (RAG) system leveraging the power of LlamaIndex. It uniquely combines a Milvus vector database for semantic search with a NebulaGraph graph database for knowledge graph-based retrieval, offering a dual-engine approach to information retrieval. The system also features multi-user knowledge base isolation, ensuring data privacy and organization.

## Key Features

*   **Advanced Document Processing**: Automatically chunks documents, extracts key entities, and generates vector embeddings upon upload.
*   **Dual-Engine Retrieval Power**: Enhances search accuracy by simultaneously querying both vector (Milvus) and graph (NebulaGraph) databases, then intelligently fusing the results.
*   **Automated Knowledge Graph Construction**: Builds a dynamic knowledge graph by identifying entities and their relationships within your documents.
*   **Multi-User Architecture**: Provides distinct Milvus collections and NebulaGraph spaces for each user, guaranteeing knowledge base separation and security.
*   **Developer-Friendly RESTful API**: Offers a standardized set of API endpoints (built with FastAPI) for seamless integration into other applications.

## System Architecture

The system is composed of the following core components:

1.  **LlamaIndex Core**: Drives document processing, indexing, retrieval, and generation.
2.  **Milvus**: Serves as the vector database, storing and querying document embeddings for semantic similarity.
3.  **NebulaGraph**: Acts as the graph database, storing and querying the knowledge graph.
4.  **FastAPI Backend**: Exposes the system's functionalities through RESTful APIs.
5.  **Multi-User Isolation Layer**: Manages separate data stores for different users.

## Getting Started

### Prerequisites

Ensure the following are installed and running:

*   Milvus (2.x)
*   NebulaGraph (3.x)
*   Python 3.8+

### Installation

1.  Clone the repository (if you haven't already):
    ```bash
    # git clone <your-repository-url>
    # cd <your-project-directory>
    ```
2.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

### Environment Configuration

Create a `.env` file in the project root (you can copy `.env.example` if provided) and configure the following:

```env
# API Configuration
API_PORT=8108
API_HOST=0.0.0.0

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key
OPENAI_MODEL=gpt-4o # Or your preferred model
OPENAI_EMBEDDING_MODEL=text-embedding-ada-002

# Milvus Configuration
MILVUS_HOST=localhost
MILVUS_PORT=19530
MILVUS_COLLECTION_PREFIX=user_ # Prefix for user-specific collections

# NebulaGraph Configuration
NEBULA_HOSTS=localhost:9669 # Comma-separated if multiple hosts
NEBULA_USER=root
NEBULA_PASSWORD=nebula
NEBULA_SPACE_PREFIX=user_knowledge_ # Prefix for user-specific graph spaces

# LlamaIndex Configuration (Adjust as needed)
CHUNK_SIZE=512
CHUNK_OVERLAP=50
TEXT_SPLITTER=sentence # Options: 'word', 'sentence', 'token', etc.
```

### Running the Application

```bash
python -m app.main
```
The API will be accessible at `http://localhost:8108`, with interactive documentation available at `http://localhost:8108/docs`.

## API Usage Examples

### Upload a Document

```bash
curl -X POST "http://localhost:8108/api/documents/upload" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@/path/to/your/document.pdf" \
  -F "user_id=user123" \
  -F "doc_name=Example Document"
```

### Perform a Query

```bash
curl -X POST "http://localhost:8108/api/query" \
  -H "accept: application/json" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user123",
    "query": "What are the main findings of this document?",
    "include_sources": true
  }'
```

## System Workflow

1.  **Document Ingestion**:
    *   User uploads a document via the API.
    *   The document is split into manageable chunks.
    *   Entities and relationships are extracted.
    *   Vector embeddings are computed for chunks.
    *   Data is indexed and stored in both Milvus (embeddings) and NebulaGraph (entities/relationships).
2.  **Query Processing**:
    *   User submits a query via the API.
    *   The query is processed by both the vector search engine (Milvus) and the graph search engine (NebulaGraph).
    *   Results from both engines are retrieved and synthesized.
    *   A consolidated answer is generated by the LLM and returned to the user.

## Project Structure

```
.
├── app/                  # Main application code
│   ├── api/              # API endpoint definitions (FastAPI routers)
│   ├── config/           # Configuration management (e.g., settings, .env loading)
│   ├── core/             # Core logic (indexing, querying, RAG pipeline)
│   ├── db/               # Database interaction (Milvus, NebulaGraph connectors)
│   ├── models/           # Pydantic models for API requests/responses and data structures
│   ├── services/         # Business logic layer
│   ├── utils/            # Utility functions
│   └── main.py           # FastAPI application entry point
├── data/                 # (Optional) Persistent data, non-versioned (e.g. local Milvus/Nebula data if not dockerized)
├── uploads/              # Temporary storage for uploaded files
├── books/                # (Example content, if applicable for sample data)
├── .env                  # Local environment variables (ignored by Git)
├── .env.example          # Example environment file
├── requirements.txt      # Python dependencies
└── README.md             # This file
```

## Technology Stack

*   **Core Framework**: LlamaIndex
*   **Web Framework**: FastAPI
*   **Vector Database**: Milvus
*   **Graph Database**: NebulaGraph
*   **LLM & Embeddings**: OpenAI (GPT models, Ada embeddings) - *Note: `transformers` and `torch` are in `requirements.txt`, suggesting flexibility for local models.*
*   **Programming Language**: Python

## Future Enhancements

*   Support for a wider range of document formats.
*   More sophisticated entity and relationship extraction.
*   Implementation of user authentication and granular permission management.
*   Advanced retrieval and result fusion strategies.
*   Development of a user-friendly web interface.

## Contributing

Contributions are welcome! Please read our `CONTRIBUTING.md` (you'll need to create this file) for guidelines on how to contribute to this project. This may include:

*   Reporting bugs
*   Suggesting enhancements
*   Submitting pull requests

## License

This project is licensed under the [Your License Here] License. See the `LICENSE` file (you'll need to create this file and choose a license like MIT, Apache 2.0, etc.) for more details.

---

*This README was last updated on 2025-05-20.* 